{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def recountsent(curfile):\n",
    "    inpfname = curfile + '/' + curfile + '-sent.xml'\n",
    "    outfname = curfile + '/' + curfile+ '-sent2.xml'\n",
    "    \n",
    "    inpf = open(inpfname)\n",
    "    outf = open(outfname,'w')\n",
    "    cursent = 0\n",
    "    for l in inpf:\n",
    "        if(re.search('n=\"1\" subtype=\"book',l)):\n",
    "            cursent = 0\n",
    "        l = re.sub('<s>','<s n=\"1111111\">',l)\n",
    "        while(re.search('<s [^>]+>',l)):\n",
    "            cursent= cursent + 1\n",
    "            l = re.sub('<s [^>]+>','<sx n=\"'+str(cursent)+'\">',l,1)\n",
    "        l = re.sub('<sx ','<s ',l)\n",
    "        print(l,end='',file=outf)\n",
    "    outf.close()\n",
    "    inpf.close()\n",
    "    \n",
    "    inpf = open(outfname)\n",
    "    outf = open(inpfname,'w')\n",
    "    for l in inpf:\n",
    "        print(l,end='',file=outf)\n",
    "    \n",
    "    inpf.close()\n",
    "    outf.close()\n",
    "    \n",
    "recountsent('xenophon_1907')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def dosents(curfile):\n",
    "    global textlines\n",
    "    inpfname = curfile + '/' + curfile + '-out.xml'\n",
    "    outfname = curfile + '/' + curfile+ '-senttmp.xml'\n",
    "    curbook = 0\n",
    "    curpage = ''\n",
    "    cursection = 1\n",
    "    curchapter = 1\n",
    "    curpage = 0\n",
    "    inpf = open(inpfname)\n",
    "    outf = open(outfname,'w')\n",
    "    innotes = 0\n",
    "    \n",
    "    cursent = 0\n",
    "    insect = 0\n",
    "    \n",
    "    for l in inpf:\n",
    "        if(re.search('n=\"1\" subtype=\"book',l)):\n",
    "            cursent = 0\n",
    "        l = re.sub('<note type=\"margin\">.+</note>','',l)\n",
    "        l = re.sub('\\n','',l)\n",
    "        if(re.search('<note type=\"footnote\">',l)):\n",
    "            innotes = 1\n",
    "        if(re.search('<pb ',l)):\n",
    "            innotes = 0\n",
    "        if(innotes):\n",
    "            print(l,file=outf)\n",
    "            continue\n",
    "            \n",
    "        if(re.search('subtype=\"section\"',l)):\n",
    "            insect = 1\n",
    "        if(re.search('</div>',l)):\n",
    "            insect = 0\n",
    "        if(insect and re.search('<p>',l)):\n",
    "            l = re.sub('<p>','<p><s>',l)\n",
    "        if(insect and re.search('</p>',l)):\n",
    "            l = re.sub('</p>','</s></p>',l)\n",
    "        if(insect):\n",
    "            \n",
    "            l = re.sub('([\\.?;][\"“0-9]*)\\s+','\\g<1></s>\\n<s>',l)\n",
    "        l = re.sub('</del></s>','</s></del>',l)\n",
    "        l = re.sub('<s><del>','<del><s>',l)\n",
    "        l = re.sub('\\]</del>“ ','</s>]</del>\" <s>',l)\n",
    "        l = re.sub(';]</del> ',';</s>]</del> <s>',l)\n",
    "        l = re.sub('<s>\\s*</s>','',l)\n",
    "        while(re.search('<s>',l)):\n",
    "            cursent = cursent+ 1\n",
    "            l = re.sub('<s>','<s n=\"'+str(cursent)+'\">',l,1)\n",
    "        print(l,file=outf)\n",
    "            \n",
    "            \n",
    "#dosents('xenophon_1907')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Cyrus, perplexed and grieved at these occurrences, sent for Clearchus; who, however, would not go, but sending a messenger to Cyrus without the knowledge of the soldiers, bade him be of good courage, as these matters would be ar- nosec \n",
      "<p>Thus they rested for that day. On the next, when they had sacrificed, and found favourable omens, and had taken their breakfast, they formed themselves in columns, ranging the Barbarians on the left in the same way, and marched forward with the archers between the columns, keep- nosec \n",
      "<p>Or if, being at the close of life, we should wish to commit to any one the guardianship of our sons, or the care of our unmarried daughters, or the preservation of our property, should we think an intemperat( man worthy of confidence for such purposes? Should we in- nosec \n",
      "<p>And what sort of mas-ters do you consider those to be, who hinder men from doing what is best, and force them to do what is worst?” \" The very worst possible, by Zeus,\" replied he. \" And what sort of slavery do you consider to be the worst ? \" \" That,\" said he, “ under the worst masters.\" \" Do not then the in- nosec \n",
      "jump 4 1 1 11 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 11 1 1 27 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 16 1 1 21 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 20 1 1 19 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 24 1 1 16 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 27 1 1 11 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 31 1 1 20 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 36 1 1 29 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 41 1 1 31 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 49 2 1 23 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 53 2 1 21 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 58 2 1 29 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 63 2 1 28 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 70 2 1 42 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 83 3 1 47 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 90 3 1 39 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 94 3 1 20 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 103 3 1 49 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 110 4 1 28 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 115 4 1 28 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 121 4 1 34 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 125 4 1 22 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 130 4 1 36 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 134 4 1 27 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 138 4 1 27 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 147 5 1 17 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 152 5 1 32 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 154 5 1 13 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 159 5 1 34 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 162 5 1 25 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 169 5 1 37 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 175 5 1 34 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 185 6 1 33 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 188 6 1 19 2 <div type=\"textpart\" n=\"2\" subtype=\"section\">\n",
      "jump 192 6 1 26 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 197 6 1 27 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 202 6 1 32 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 215 7 1 41 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 221 7 1 38 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 229 7 1 48 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 233 7 1 24 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 236 7 1 16 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 244 7 1 44 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 253 7 1 57 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 354 1 1 20 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 368 1 1 64 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 373 1 1 15 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 378 1 1 19 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 380 1 1 6 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 384 1 1 15 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 396 2 1 34 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 400 2 1 14 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 404 2 1 18 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 405 2 1 7 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 406 2 1 5 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 414 2 1 39 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 419 2 1 13 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 420 2 1 6 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 422 2 1 8 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 426 3 1 11 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 427 3 1 4 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 430 3 1 14 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 433 3 1 12 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 440 3 1 28 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 444 3 1 18 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 446 3 1 9 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 450 3 1 10 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 454 3 1 15 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 457 3 1 14 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 462 3 1 16 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 464 3 1 8 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 466 3 1 6 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 470 4 1 5 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 482 4 1 40 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 487 4 1 18 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 494 4 1 24 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 497 4 1 12 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 501 4 1 15 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 504 4 1 10 1 <div type=\"textpart\" n=\"1\" subtype=\"section\">\n",
      "jump 518 2 <pb n=\"2\"/>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokenlist = {}\n",
    "textlines = []\n",
    "\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "#spell = SpellChecker()\n",
    "\n",
    "#print('installed')\n",
    "\n",
    "def makedict(fname):\n",
    "    global tokenlist\n",
    "    f = open(fname)\n",
    "    \n",
    "    for l in f:\n",
    "    \n",
    "        l = re.sub('\\s+$','',l)\n",
    "        l = re.sub('­','-',l)\n",
    "        if(re.search('­',l)):\n",
    "            print('hyph',l)\n",
    "        textlines.append(l)\n",
    "        l = re.sub('<[^>]+>',' ',l)\n",
    "        l = re.sub('[\\.;:!,?!”\"\"0-9]', ' ',l)\n",
    "        l = re.sub('^\\s*[^\\s+]+\\s+','',l)\n",
    "        l = re.sub('\\s+[^\\s]+\\s*$','',l)\n",
    "        for foo in l.split():\n",
    "            if(foo in tokenlist):\n",
    "                tokenlist[foo] = tokenlist[foo] + 1\n",
    "            else:\n",
    "                tokenlist[foo] = 1\n",
    "        #print(l)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    i = 0\n",
    "    hits = 0\n",
    "    for foo in textlines:\n",
    "        i = i + 1\n",
    "        if(i>len(textlines)-1):\n",
    "            continue\n",
    "        m = re.search('\\s+([a-zA-Z]+[a-z])\\s*$',foo)\n",
    "        n = re.search('^\\s*([a-z]+)',textlines[i])\n",
    "        if(m and n):\n",
    "            half1 = m[1]\n",
    "            half2 = n[1]\n",
    "            testword = half1 + half2\n",
    "            #tool.correct('A sentence with a error in the Hitchhiker’s Guide tot he Galaxy')\n",
    "            #if(not spell.unknown([testword])):\n",
    "            #    print(i,'spell',testword)\n",
    "\n",
    "            if(testword in tokenlist):\n",
    "                hits = hits + 1\n",
    "                textlines[i-1] = foo + '-'\n",
    "\n",
    "                #print(i,hits,half1+half2,foo)\n",
    "    i = 0\n",
    "    hits = 0\n",
    "    for foo in textlines:\n",
    "        i = i + 1\n",
    "        if(re.search('-$',foo)):\n",
    "            m = re.search('^(\\s*)([a-z][^\\s]+)\\s*',textlines[i])\n",
    "            if(m):\n",
    "                hits = hits + 1\n",
    "                half2 = m[2]\n",
    "                textlines[i-1] = re.sub('-$',half2,textlines[i-1])\n",
    "                textlines[i] = re.sub('^(\\s*)([a-z][^\\s]+)\\s*','\\g<1>',textlines[i])\n",
    "\n",
    "                #print(i,hits,'hyph',foo+half2)\n",
    "            else:\n",
    "                print(foo,'nosec',textlines[i])\n",
    "            \n",
    "\n",
    "def dochapsects(curfile):\n",
    "    global textlines\n",
    "    inpfname = curfile + '/' + curfile + '.xml'\n",
    "    outfname = curfile + '/' + curfile+ '-outx.xml'\n",
    "    makedict(inpfname)\n",
    "    curbook = 0\n",
    "    curpage = ''\n",
    "    cursection = 1\n",
    "    curchapter = 1\n",
    "    curpage = 0\n",
    "    inpf = open(inpfname)\n",
    "    outf = open(outfname,'w')\n",
    "    innotes = 0\n",
    "    for l in textlines:\n",
    "        l = re.sub('Æ','Ae',l)\n",
    "        if(re.search('<note type=\"footnote\">',l)):\n",
    "            innotes = 1\n",
    "        if(re.search('<pb ',l)):\n",
    "            innotes = 0\n",
    "        if(innotes):\n",
    "            print(l,file=outf)\n",
    "            continue\n",
    "        l = re.sub('(\\\\bsect\\.)\\s+([0-9])','\\g<1>-\\g<2>',l)\n",
    "        \n",
    "        m = re.search('<pb n=\"([0-9]+)\"',l)\n",
    "        if(m):\n",
    "            workpage = int(m[1])\n",
    "            if(curpage and not workpage == curpage + 1):\n",
    "                print('jump',curpage,workpage,l)\n",
    "            curpage = workpage\n",
    "        if((curpage > 258 and curpage < 349) or curpage > 507 ):\n",
    "            print(l,file=outf)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #m = re.search('<pb n=\"([0-9]+)\"/>',l)\n",
    "        #if(m):\n",
    "       #     curpage = m[1]\n",
    "        if(re.search('n=\"1\" subtype=\"chapter\"',l)):\n",
    "            curchapter = 1\n",
    "        \n",
    "        m = re.search('n=\"([0-9])\" subtype=\"book\"',l)\n",
    "        if(m):\n",
    "            curbook = int(m[1])\n",
    "            cursection = 0\n",
    "        l = re.sub('([a-z][a-z]\\s*[?\"!\\.;:!\\)\\’]+|[a-z][a-z]\\s*[?\"!\\.;”:!\\)\\’]+\\s*[0-9”\"]|^)(\\s+|^)([0-9]+)\\.(\\s+|\\n|$)','\\g<1></p>\\n</div>\\n\\n<div type=\"textpart\" n=\"\\g<3>\" subtype=\"section\">\\n<p>',l)\n",
    "        \n",
    "        \n",
    "        m = re.search('n=\"([0-9]+)\" subtype=\"section\"',l)\n",
    "        if(m):\n",
    "            worksection = int(m[1])\n",
    "            if(not worksection == cursection + 1):\n",
    "                print('jump',curpage,curbook,curchapter,cursection,worksection,l)\n",
    "            cursection = worksection\n",
    "            \n",
    "        if(re.search('n=\"Z\" subtype=\"chapter\"',l)):\n",
    "            curchapter = curchapter + 1\n",
    "            cursection = 0\n",
    "            l = re.sub('n=\"Z\" subtype=\"chapter\"','n=\"'+str(curchapter)+'\" subtype=\"chapter\"',l)\n",
    "        l = re.sub('(\\\\bsect\\.)-([0-9])','\\g<1> \\g<2>',l)\n",
    "        print(l,file=outf)\n",
    "        \n",
    "        \n",
    "dochapsects('xenophon_1907')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenlist['Hercules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6d1d5f926844f5be0db21a8f13c906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:18:39 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1559464405b40718194875f6328de56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:19:20 INFO: Finished downloading models and saved to /Users/gcrane/stanza_resources.\n",
      "2023-03-06 17:19:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fdd97f966a5432ca9f2b3ab90be84af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:19:22 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-03-06 17:19:22 INFO: Use device: cpu\n",
      "2023-03-06 17:19:22 INFO: Loading: tokenize\n",
      "2023-03-06 17:19:22 INFO: Loading: pos\n",
      "2023-03-06 17:19:22 INFO: Loading: lemma\n",
      "2023-03-06 17:19:22 INFO: Loading: depparse\n",
      "2023-03-06 17:19:22 INFO: Loading: sentiment\n",
      "2023-03-06 17:19:23 INFO: Loading: constituency\n",
      "2023-03-06 17:19:23 INFO: Loading: ner\n",
      "2023-03-06 17:19:24 INFO: Done loading processors!\n",
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en') # download English model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:19:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dd7f42e55e4d5ab8cb0780f94f224e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:19:48 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-03-06 17:19:48 INFO: Use device: cpu\n",
      "2023-03-06 17:19:48 INFO: Loading: tokenize\n",
      "2023-03-06 17:19:48 INFO: Loading: pos\n",
      "2023-03-06 17:19:49 INFO: Loading: lemma\n",
      "2023-03-06 17:19:49 INFO: Loading: depparse\n",
      "2023-03-06 17:19:49 INFO: Loading: sentiment\n",
      "2023-03-06 17:19:49 INFO: Loading: constituency\n",
      "2023-03-06 17:19:49 INFO: Loading: ner\n",
      "2023-03-06 17:19:50 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\") # run annotation over a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc\n",
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:21:43 INFO: Writing properties to tmp file: corenlp_server-7a3ea4605daa4826.props\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Please install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define $CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter.  $CORENLP_HOME=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mCoreNLPClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mssplit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlemma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m4G\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:9001\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbe_quiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(client)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Start the background server and wait for some time\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/stanza/server/client.py:306\u001b[0m, in \u001b[0;36mCoreNLPClient.__init__\u001b[0;34m(self, start_server, endpoint, timeout, threads, annotators, pretokenized, output_format, properties, stdout, stderr, memory, be_quiet, max_char_length, preload, classpath, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(port)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m host \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf starting a server, endpoint must be localhost\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 306\u001b[0m classpath \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_classpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasspath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m start_cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava -Xmx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -cp \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasspath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  edu.stanford.nlp.pipeline.StanfordCoreNLPServer \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    308\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-port \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -timeout \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -threads \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -maxCharLength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_char_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    309\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-quiet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbe_quiet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_classpath \u001b[38;5;241m=\u001b[39m classpath\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/stanza/server/client.py:241\u001b[0m, in \u001b[0;36mresolve_classpath\u001b[0;34m(classpath)\u001b[0m\n\u001b[1;32m    238\u001b[0m     classpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCORENLP_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(Path\u001b[38;5;241m.\u001b[39mhome()), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstanza_corenlp\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(classpath):\n\u001b[0;32m--> 241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m                                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$CORENLP_HOME=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCORENLP_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    244\u001b[0m     classpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(classpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classpath\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Please install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define $CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter.  $CORENLP_HOME=None"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
    "client = CoreNLPClient(\n",
    "    annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], \n",
    "    memory='4G', \n",
    "    endpoint='http://localhost:9001',\n",
    "    be_quiet=True)\n",
    "print(client)\n",
    "\n",
    "# Start the background server and wait for some time\n",
    "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
    "client.start()\n",
    "import time; time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-d7bd9598d58d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m'6      οὐλομένην       οὐλόμενος       ADJ     a-s---fa-       Case=Acc|Gender=Fem|Number=Sing 1       amod    _       Ref=1.2|SpaceAfter=No'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m ]\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mdisplay_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-d7bd9598d58d>\u001b[0m in \u001b[0;36mdisplay_tree\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Split the input data into lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Parse the tree data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "def display_tree(data):\n",
    "    # Split the input data into lines\n",
    "    #lines = data.strip().split('\\n')\n",
    "    lines = data\n",
    "    \n",
    "    # Parse the tree data\n",
    "    tree = {}\n",
    "    for line in lines:\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 8:\n",
    "            idx, word, lemma, pos, _, head, dep, _ = parts[:8]\n",
    "            tree[int(idx)] = {'word': word, 'lemma': lemma, 'pos': pos, 'head': int(head), 'dep': dep}\n",
    "    \n",
    "    # Function to recursively print the tree\n",
    "    def print_tree(node, depth):\n",
    "        print('  ' * depth + f\"{node['word']} ({node['lemma']}, {node['pos']}, {node['dep']})\")\n",
    "        for child_idx, child in tree.items():\n",
    "            if child['head'] == node['idx']:\n",
    "                print_tree(child, depth + 1)\n",
    "    \n",
    "    # Find the root of the tree and print the tree\n",
    "    for idx, node in tree.items():\n",
    "        if node['head'] == 0:\n",
    "            node['idx'] = idx\n",
    "            print_tree(node, 0)\n",
    "            break\n",
    "\n",
    "# Example usage\n",
    "data = [\n",
    "'1\tμῆνιν\tμῆνις\tNOUN\tn-s---fa-\tCase=Acc|Gender=Fem|Number=Sing\t2\tobj\t_\tRef=1.1',\n",
    "'2\tἄειδε\tἀείδω\tVERB\tv2spma---\tMood=Imp|Number=Sing|Person=2|Tense=Pres|Voice=Act\t0\troot\t_\tRef=1.1',\n",
    "'3\tθεὰ\tθεά\tNOUN\tn-s---fv-\tCase=Voc|Gender=Fem|Number=Sing\t2\tvocative\t_\tRef=1.1',\n",
    "'4\tΠηληϊάδεω\tΠηληιάδης\tPROPN\tn-s---mg-\tCase=Gen|Gender=Masc|Number=Sing\t5\tnmod\t_\tRef=1.1',\n",
    "'5\tἈχιλῆος\tἈχιλλεύς\tPROPN\tn-s---mg-\tCase=Gen|Gender=Masc|Number=Sing\t1\tnmod\t_\tRef=1.1',\n",
    "'6\tοὐλομένην\tοὐλόμενος\tADJ\ta-s---fa-\tCase=Acc|Gender=Fem|Number=Sing\t1\tamod\t_\tRef=1.2|SpaceAfter=No'\n",
    "]\n",
    "display_tree(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
